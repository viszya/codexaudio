#!/usr/bin/env python3
"""
dual_input_transcription.py  â€“ built-in mic + BlackHole (Aggregate 3-ch)
-----------------------------------------------------------------------
Person 1 â†’ channel 0  (built-in mic, mono)  â†’  person1.txt
Person 2 â†’ channels 1 & 2 averaged          â†’  person2.txt
"""
from __future__ import annotations
import sys, ctypes
from Foundation import NSRunLoop, NSLocale
from AVFoundation import *
from Speech import *
import pyaudio

TARGET_AGGREGATE_NAME = "System + Mic"      # â† change if your Aggregateâ€™s name differs
BUFFER_SIZE           = 1024                # frames per tap
SAMPLE_RATE           = 44_100.0            # Hz

# â”€â”€ 1. Mic permission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
session = AVAudioSession.sharedInstance()
# if not session.requestRecordPermission_(lambda g: None):
#     print("âŒ Microphone permission denied."); sys.exit(1)

# â”€â”€ 2. Locate Aggregate device via PyAudio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
p = pyaudio.PyAudio()
agg_index = next( (i for i in range(p.get_device_count())
                   if TARGET_AGGREGATE_NAME in p.get_device_info_by_index(i)["name"]),
                  None )
if agg_index is None:
    print(f"âŒ Aggregate device '{TARGET_AGGREGATE_NAME}' not found."); sys.exit(1)
agg_name = p.get_device_info_by_index(agg_index)["name"]
print(f"âœ“ Using aggregate device: {agg_name}")

# â”€â”€ 3. Configure AVAudioSession category & *attempt* preferred input â”€â”€â”€â”€â”€â”€â”€
session.setCategory_error_(AVAudioSessionCategoryPlayAndRecord, None)
session.setActive_error_(True, None)

matched = False
print("\nğŸ” AVAudioSession inputs:")
for port in session.availableInputs() or []:
    print(f"   â€¢ {port.portName():<30} ({port.portType()})")
    if port.portName() == agg_name:
        matched = session.setPreferredInput_error_(port, None) is None
if not matched:
    print("âš ï¸  Aggregate device not in AVAudioSession; continuing with system default.\n"
          "   Pick it manually in System Settings â–¸ Sound â–¸ Input to force it.")

# â”€â”€ 4. Prepare two Speech recognizers & requests â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
locale = NSLocale.localeWithLocaleIdentifier_("en-US")
rec1, rec2 = (SFSpeechRecognizer.alloc().initWithLocale_(locale) for _ in range(2))
if not (rec1.isAvailable() and rec2.isAvailable()):
    print("âŒ Speech recognition unavailable."); sys.exit(1)

req1, req2 = SFSpeechAudioBufferRecognitionRequest.new(), SFSpeechAudioBufferRecognitionRequest.new()
req1.shouldReportPartialResults = True
req2.shouldReportPartialResults = True
f1, f2 = (open(fname, "w+", encoding="utf-8") for fname in ("person1.txt", "person2.txt"))

def mk_handler(fp):
    def _handler(result, error):
        if result:
            fp.seek(0); fp.write(result.bestTranscription().formattedString())
            fp.truncate(); fp.flush()
        elif error:
            # Ignore â€œno speechâ€ warnings; show others
            if error.code() != 1110:
                print("Recognition error:", error.localizedDescription())
    return _handler

rec1.recognitionTaskWithRequest_resultHandler_(req1, mk_handler(f1))
rec2.recognitionTaskWithRequest_resultHandler_(req2, mk_handler(f2))

# â”€â”€ 5. Helper: convert PyObjC channel pointer â†’ raw int address â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def addr(channel_ptr) -> int:
    """Return integer (void*) address from PyObjC varlist element."""
    return ctypes.cast(channel_ptr, ctypes.c_void_p).value

# â”€â”€ 6. Copy / average helpers (operate on raw addresses) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def memcpy_f32(dst_addr, src_addr, frames):
    ctypes.memmove(dst_addr, src_addr, frames * 4)      # 4 bytes / float32

def avg_f32(dst_addr, src1_addr, src2_addr, frames):
    src1 = (ctypes.c_float * frames).from_address(src1_addr)
    src2 = (ctypes.c_float * frames).from_address(src2_addr)
    dst  = (ctypes.c_float * frames).from_address(dst_addr)
    for i in range(frames):
        dst[i] = 0.5 * (src1[i] + src2[i])

# â”€â”€ 7. AVAudioEngine tap (3-ch â†’ two mono buffers) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
engine     = AVAudioEngine.alloc().init()
input_node = engine.inputNode()
fmt3       = input_node.outputFormatForBus_(0)
if fmt3.channelCount() != 3:
    print("âŒ Expected 3-channel input; found", fmt3.channelCount()); sys.exit(1)

mono_fmt = AVAudioFormat.alloc().initStandardFormatWithSampleRate_channels_(SAMPLE_RATE, 1)

def tap_cb(buffer, when):
    frames   = buffer.frameLength()
    ch_data  = buffer.floatChannelData()
    ch0_ptr  = addr(ch_data[0])
    ch1_ptr  = addr(ch_data[1])
    ch2_ptr  = addr(ch_data[2])

    # Person 1 (built-in mic)
    buf1 = AVAudioPCMBuffer.alloc().initWithPCMFormat_frameCapacity_(mono_fmt, frames)
    buf1.frameLength = frames
    memcpy_f32(addr(buf1.floatChannelData()[0]), ch0_ptr, frames)
    req1.appendAudioPCMBuffer_(buf1)

    # Person 2 (BlackHole stereo â†’ mono)
    buf2 = AVAudioPCMBuffer.alloc().initWithPCMFormat_frameCapacity_(mono_fmt, frames)
    buf2.frameLength = frames
    avg_f32(addr(buf2.floatChannelData()[0]), ch1_ptr, ch2_ptr, frames)
    req2.appendAudioPCMBuffer_(buf2)

input_node.installTapOnBus_bufferSize_format_block_(0, BUFFER_SIZE, fmt3, tap_cb)
engine.prepare(); engine.startAndReturnError_(None)

print("\nğŸ¤  Listening â€¦")
print("    â€¢ Person 1 â†’ built-in mic  â†’ person1.txt")
print("    â€¢ Person 2 â†’ BlackHole     â†’ person2.txt")
NSRunLoop.currentRunLoop().run()
